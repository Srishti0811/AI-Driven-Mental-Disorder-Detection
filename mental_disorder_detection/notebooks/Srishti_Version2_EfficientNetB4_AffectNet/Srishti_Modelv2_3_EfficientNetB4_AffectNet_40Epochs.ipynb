{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "4PnPgfBpXu7L",
    "outputId": "2913c8bd-327a-4e46-dbcf-7d31c3c3f317"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-b4fdc32d-6277-476a-9532-5b65a0316371\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-b4fdc32d-6277-476a-9532-5b65a0316371\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kaggle.json': b'{\"username\":\"srishtidixit0811\",\"key\":\"1a3e778ca6d47e5f829497a31cb2aea1\"}'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIhoycGZYLtb",
    "outputId": "c3fed5b7-53e5-4398-8794-6da9be49ed99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5kKIzNEUYOoX"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cijguRa_YQIe",
    "outputId": "0d2319b4-6818-4aff-adbe-637bf1bbf841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/thienkhonghoc/affectnet\n",
      "License(s): unknown\n",
      "Downloading affectnet.zip to /content\n",
      " 84% 1.47G/1.75G [01:07<00:12, 23.6MB/s]"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d thienkhonghoc/affectnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SQWLpGmYWwb"
   },
   "outputs": [],
   "source": [
    "!unzip -q /content/affectnet.zip -d /content/affectnet > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Jh9qaaFYud5"
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe opencv-python numpy pandas tensorflow keras albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPjDA13ddBIR"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade mediapipe opencv-python tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvzYnEd4dHuG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPTQ9OMSYWBl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Enable GPU processing\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU is enabled and available.\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"GPU setup error:\", e)\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")\n",
    "\n",
    "# Set Correct Paths\n",
    "DATASET_PATH = \"/content/affectnet/AffectNet/\"  # Main dataset directory\n",
    "PROCESSED_PATH = \"/content/AffectNet_Preprocessed/\"  # Where processed images will be saved\n",
    "BATCH_SIZE = 5000  # Reduce batch size if Colab runs out of memory\n",
    "\n",
    "# Create directory for processed images\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize Mediapipe Face Detector\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.7)\n",
    "\n",
    "# Function to detect and process faces in batches using GPU\n",
    "@tf.function  # JIT compilation for GPU acceleration\n",
    "def process_batch(batch_files, label, subset, save_path):\n",
    "    for img_file in tqdm(batch_files, desc=f\"Processing {subset}/{label}\"):\n",
    "        img_path = os.path.join(DATASET_PATH, subset, label, img_file)\n",
    "        save_subset_path = os.path.join(save_path, subset, label)\n",
    "        os.makedirs(save_subset_path, exist_ok=True)  # Ensure subset directories exist\n",
    "\n",
    "        save_file_path = os.path.join(save_subset_path, img_file)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue  # Skip if image is not valid\n",
    "\n",
    "        # Convert to RGB and process with Mediapipe\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(img_rgb)\n",
    "\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                h, w, _ = img.shape\n",
    "                x, y, w, h = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)\n",
    "\n",
    "                # Ensure face bounding box is valid\n",
    "                if x < 0 or y < 0 or w <= 10 or h <= 10:\n",
    "                    continue  # Skip images with invalid face detection\n",
    "\n",
    "                face = img[y:y + h, x:x + w]\n",
    "\n",
    "                # Ensure the cropped face is not empty\n",
    "                if face is None or face.shape[0] == 0 or face.shape[1] == 0:\n",
    "                    continue  # Skip empty images\n",
    "\n",
    "                # Use OpenCV CUDA for GPU-accelerated resizing\n",
    "                if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n",
    "                    gpu_img = cv2.cuda_GpuMat()\n",
    "                    gpu_img.upload(face)\n",
    "                    gpu_img = cv2.cuda.resize(gpu_img, (224, 224))\n",
    "                    face = gpu_img.download()\n",
    "                else:\n",
    "                    face = cv2.resize(face, (224, 224))\n",
    "\n",
    "                # Convert to grayscale\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Save processed image\n",
    "                cv2.imwrite(save_file_path, face)\n",
    "\n",
    "# Process images in batches for train, val, and test sets\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    subset_path = os.path.join(DATASET_PATH, subset)\n",
    "\n",
    "    if not os.path.exists(subset_path):\n",
    "        print(f\"Warning: {subset_path} does not exist. Skipping...\")\n",
    "        continue  # Skip if directory does not exist\n",
    "\n",
    "    for label in os.listdir(subset_path):\n",
    "        label_path = os.path.join(subset_path, label)\n",
    "        save_label_path = os.path.join(PROCESSED_PATH, subset, label)\n",
    "\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue  # Skip files, only process directories\n",
    "\n",
    "        image_files = os.listdir(label_path)\n",
    "\n",
    "        # Process in chunks of BATCH_SIZE\n",
    "        for i in range(0, len(image_files), BATCH_SIZE):\n",
    "            batch_files = image_files[i:i + BATCH_SIZE]\n",
    "            process_batch(batch_files, label, subset, PROCESSED_PATH)\n",
    "            print(f\"Processed {len(batch_files)} images in batch for {subset}/{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeejkbVGh1td"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z25DL8ejlKDk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROCESSED_PATH = \"/content/AffectNet_Preprocessed/train/\"\n",
    "\n",
    "class_counts = {label: len(os.listdir(os.path.join(PROCESSED_PATH, label))) for label in os.listdir(PROCESSED_PATH)}\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xlabel(\"Emotion Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Class Distribution in AffectNet\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(\"Class Distribution:\", class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qR75Ai04B-_"
   },
   "outputs": [],
   "source": [
    "#v4.1\n",
    "#Increasing epochs from 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evx1C9mahdaO",
    "outputId": "780acffa-5170-4484-d23f-c9a72a7f15b3"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GPU status...\n",
      "Wed Mar  5 19:22:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   56C    P0             29W /   70W |     102MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "GPU is enabled.\n",
      "Found 36028 images belonging to 8 classes.\n",
      "Found 778 images belonging to 8 classes.\n",
      "Classes: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Loading previous checkpoint...\n",
      "Resuming training from epoch 31 to 35\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/35\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m828s\u001b[0m 295ms/step - accuracy: 0.5179 - loss: 1.2851 - val_accuracy: 0.5514 - val_loss: 1.2520 - learning_rate: 2.5000e-06\n",
      "Epoch 32/35\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m544s\u001b[0m 241ms/step - accuracy: 0.5233 - loss: 1.2762 - val_accuracy: 0.5476 - val_loss: 1.2402 - learning_rate: 2.5000e-06\n",
      "Epoch 33/35\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 240ms/step - accuracy: 0.5242 - loss: 1.2721 - val_accuracy: 0.5463 - val_loss: 1.2487 - learning_rate: 2.5000e-06\n",
      "Epoch 34/35\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.5259 - loss: 1.2676\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 240ms/step - accuracy: 0.5259 - loss: 1.2676 - val_accuracy: 0.5476 - val_loss: 1.2638 - learning_rate: 2.5000e-06\n",
      "Epoch 35/35\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 238ms/step - accuracy: 0.5276 - loss: 1.2678 - val_accuracy: 0.5424 - val_loss: 1.2413 - learning_rate: 1.2500e-06\n",
      "Resuming training from epoch 36 to 40\n",
      "Epoch 36/40\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 242ms/step - accuracy: 0.5233 - loss: 1.2730 - val_accuracy: 0.5566 - val_loss: 1.2453 - learning_rate: 1.2500e-06\n",
      "Epoch 37/40\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 238ms/step - accuracy: 0.5253 - loss: 1.2686 - val_accuracy: 0.5501 - val_loss: 1.2502 - learning_rate: 1.2500e-06\n",
      "Epoch 38/40\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.5235 - loss: 1.2676\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 239ms/step - accuracy: 0.5235 - loss: 1.2676 - val_accuracy: 0.5463 - val_loss: 1.2471 - learning_rate: 1.2500e-06\n",
      "Epoch 39/40\n",
      "\u001b[1m1449/2252\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3:09\u001b[0m 237ms/step - accuracy: 0.5224 - loss: 1.2772"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Check GPU Status\n",
    "print(\"Checking GPU status...\")\n",
    "!nvidia-smi\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"GPU is enabled.\")\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")\n",
    "\n",
    "# Enable mixed precision for speedup\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "PROCESSED_PATH = \"/content/AffectNet_Preprocessed/\"\n",
    "checkpoint_path = \"/content/best_augmented_model_4.2.keras\"  # Load the saved model\n",
    "\n",
    "# Data Augmentation (Enhance Variability)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "# Load Dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PROCESSED_PATH + \"train\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    PROCESSED_PATH + \"val\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "print(\"Classes:\", class_labels)\n",
    "\n",
    "# Load Saved Model from Epoch 30\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading previous checkpoint...\")\n",
    "    model = load_model(checkpoint_path)\n",
    "    epochs_completed = 30  # Resume from epoch 31\n",
    "else:\n",
    "    print(\"No checkpoint found. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Define Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor=\"val_accuracy\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Resume Training in Batches of 5 Epochs\n",
    "epochs_completed = 30  # Start from 30th epoch\n",
    "max_epochs = 50  # Train until 50 epochs max\n",
    "\n",
    "while epochs_completed < max_epochs:\n",
    "    print(f\"Resuming training from epoch {epochs_completed + 1} to {epochs_completed + 5}\")\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs_completed + 5,\n",
    "        initial_epoch=epochs_completed,\n",
    "        callbacks=[early_stop, checkpoint, reduce_lr]\n",
    "    )\n",
    "\n",
    "    # Save Model Progress After Every 5 Epochs\n",
    "    model.save(checkpoint_path)\n",
    "\n",
    "    epochs_completed += 5\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
