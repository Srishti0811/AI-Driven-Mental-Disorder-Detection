{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "4PnPgfBpXu7L",
    "outputId": "bc83f169-ccb0-4732-9a4a-db52284592ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-abd07880-daa9-4208-b34c-f9afad6fc8b0\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-abd07880-daa9-4208-b34c-f9afad6fc8b0\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kaggle.json': b'{\"username\":\"srishtidixit0811\",\"key\":\"1a3e778ca6d47e5f829497a31cb2aea1\"}'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIhoycGZYLtb",
    "outputId": "29c4e879-d19e-4d83-9fc5-3d5a96feb2a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5kKIzNEUYOoX"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cijguRa_YQIe",
    "outputId": "d5e1036a-7bbb-48e3-da2a-0f9031b3ce7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/thienkhonghoc/affectnet\n",
      "License(s): unknown\n",
      "Downloading affectnet.zip to /content\n",
      "100% 1.74G/1.75G [00:48<00:00, 41.8MB/s]\n",
      "100% 1.75G/1.75G [00:48<00:00, 38.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d thienkhonghoc/affectnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4SQWLpGmYWwb"
   },
   "outputs": [],
   "source": [
    "!unzip -q /content/affectnet.zip -d /content/affectnet > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Jh9qaaFYud5",
    "outputId": "a973e581-943e-4bfa-a08d-bcf3a0db54b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.4)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.13.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.10.6)\n",
      "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.12.1)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: sounddevice, mediapipe\n",
      "Successfully installed mediapipe-0.10.21 sounddevice-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python numpy pandas tensorflow keras albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPjDA13ddBIR",
    "outputId": "7c6df333-99c9-4e61-aec2-ea3862f89a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade mediapipe opencv-python tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvzYnEd4dHuG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPTQ9OMSYWBl",
    "outputId": "8978b271-d1ee-4952-f892-3ce0187940ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled and available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing train/3:   0%|          | 0/5000 [00:00<?, ?it/s]WARNING:tensorflow:AutoGraph could not transform <function makedirs at 0x7d8b72ff7d80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function makedirs at 0x7d8b72ff7d80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function makedirs at 0x7d8b72ff7d80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function makedirs at 0x7d8b72ff7d80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train/3: 100%|██████████| 5000/5000 [01:19<00:00, 62.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 images in batch for train/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train/6: 100%|██████████| 5000/5000 [00:59<00:00, 83.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 images in batch for train/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train/0: 100%|██████████| 5000/5000 [00:53<00:00, 93.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 images in batch for train/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train/4: 100%|██████████| 5000/5000 [00:55<00:00, 89.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 images in batch for train/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train/1: 100%|██████████| 3803/3803 [00:40<00:00, 92.83it/s]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function process_batch at 0x7d8aba5a8c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3803 images in batch for train/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train/5: 100%|██████████| 5000/5000 [00:59<00:00, 84.00it/s]\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function process_batch at 0x7d8aba5a8c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 images in batch for train/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train/7: 100%|██████████| 3750/3750 [00:43<00:00, 86.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3750 images in batch for train/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train/2: 100%|██████████| 5000/5000 [00:54<00:00, 91.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 images in batch for train/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val/3: 100%|██████████| 100/100 [00:01<00:00, 84.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images in batch for val/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val/6: 100%|██████████| 100/100 [00:01<00:00, 87.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images in batch for val/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val/0: 100%|██████████| 100/100 [00:01<00:00, 92.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images in batch for val/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val/4: 100%|██████████| 100/100 [00:01<00:00, 94.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images in batch for val/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val/1: 100%|██████████| 100/100 [00:01<00:00, 91.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images in batch for val/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val/5: 100%|██████████| 100/100 [00:01<00:00, 97.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images in batch for val/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val/7: 100%|██████████| 100/100 [00:01<00:00, 78.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images in batch for val/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val/2: 100%|██████████| 100/100 [00:01<00:00, 94.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images in batch for val/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test/3: 100%|██████████| 400/400 [00:05<00:00, 72.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 images in batch for test/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test/6: 100%|██████████| 400/400 [00:04<00:00, 81.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 images in batch for test/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test/0: 100%|██████████| 400/400 [00:04<00:00, 81.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 images in batch for test/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test/4: 100%|██████████| 400/400 [00:04<00:00, 92.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 images in batch for test/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test/1: 100%|██████████| 400/400 [00:04<00:00, 92.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 images in batch for test/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test/5: 100%|██████████| 400/400 [00:04<00:00, 81.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 images in batch for test/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test/7: 100%|██████████| 400/400 [00:04<00:00, 87.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 images in batch for test/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test/2: 100%|██████████| 400/400 [00:04<00:00, 91.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400 images in batch for test/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Enable GPU processing\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU is enabled and available.\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"GPU setup error:\", e)\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")\n",
    "\n",
    "# Set Correct Paths\n",
    "DATASET_PATH = \"/content/affectnet/AffectNet/\"  # Main dataset directory\n",
    "PROCESSED_PATH = \"/content/AffectNet_Preprocessed/\"  # Where processed images will be saved\n",
    "BATCH_SIZE = 5000  # Reduce batch size if Colab runs out of memory\n",
    "\n",
    "# Create directory for processed images\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize Mediapipe Face Detector\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.7)\n",
    "\n",
    "# Function to detect and process faces in batches using GPU\n",
    "@tf.function  # JIT compilation for GPU acceleration\n",
    "def process_batch(batch_files, label, subset, save_path):\n",
    "    for img_file in tqdm(batch_files, desc=f\"Processing {subset}/{label}\"):\n",
    "        img_path = os.path.join(DATASET_PATH, subset, label, img_file)\n",
    "        save_subset_path = os.path.join(save_path, subset, label)\n",
    "        os.makedirs(save_subset_path, exist_ok=True)  # Ensure subset directories exist\n",
    "\n",
    "        save_file_path = os.path.join(save_subset_path, img_file)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue  # Skip if image is not valid\n",
    "\n",
    "        # Convert to RGB and process with Mediapipe\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(img_rgb)\n",
    "\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                h, w, _ = img.shape\n",
    "                x, y, w, h = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)\n",
    "\n",
    "                # Ensure face bounding box is valid\n",
    "                if x < 0 or y < 0 or w <= 10 or h <= 10:\n",
    "                    continue  # Skip images with invalid face detection\n",
    "\n",
    "                face = img[y:y + h, x:x + w]\n",
    "\n",
    "                # Ensure the cropped face is not empty\n",
    "                if face is None or face.shape[0] == 0 or face.shape[1] == 0:\n",
    "                    continue  # Skip empty images\n",
    "\n",
    "                # Use OpenCV CUDA for GPU-accelerated resizing\n",
    "                if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n",
    "                    gpu_img = cv2.cuda_GpuMat()\n",
    "                    gpu_img.upload(face)\n",
    "                    gpu_img = cv2.cuda.resize(gpu_img, (224, 224))\n",
    "                    face = gpu_img.download()\n",
    "                else:\n",
    "                    face = cv2.resize(face, (224, 224))\n",
    "\n",
    "                # Convert to grayscale\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Save processed image\n",
    "                cv2.imwrite(save_file_path, face)\n",
    "\n",
    "# Process images in batches for train, val, and test sets\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    subset_path = os.path.join(DATASET_PATH, subset)\n",
    "\n",
    "    if not os.path.exists(subset_path):\n",
    "        print(f\"Warning: {subset_path} does not exist. Skipping...\")\n",
    "        continue  # Skip if directory does not exist\n",
    "\n",
    "    for label in os.listdir(subset_path):\n",
    "        label_path = os.path.join(subset_path, label)\n",
    "        save_label_path = os.path.join(PROCESSED_PATH, subset, label)\n",
    "\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue  # Skip files, only process directories\n",
    "\n",
    "        image_files = os.listdir(label_path)\n",
    "\n",
    "        # Process in chunks of BATCH_SIZE\n",
    "        for i in range(0, len(image_files), BATCH_SIZE):\n",
    "            batch_files = image_files[i:i + BATCH_SIZE]\n",
    "            process_batch(batch_files, label, subset, PROCESSED_PATH)\n",
    "            print(f\"Processed {len(batch_files)} images in batch for {subset}/{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeejkbVGh1td",
    "outputId": "5b7d779f-d415-433a-c229-bcf6a60f05b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "Z25DL8ejlKDk",
    "outputId": "b6b84c87-b25c-4da4-f750-c0b823dce50e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution: {'3': 4837, '6': 4803, '0': 4738, '4': 4741, '1': 3635, '5': 4843, '7': 3625, '2': 4806}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROCESSED_PATH = \"/content/AffectNet_Preprocessed/train/\"\n",
    "\n",
    "class_counts = {label: len(os.listdir(os.path.join(PROCESSED_PATH, label))) for label in os.listdir(PROCESSED_PATH)}\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xlabel(\"Emotion Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Class Distribution in AffectNet\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(\"Class Distribution:\", class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qR75Ai04B-_"
   },
   "outputs": [],
   "source": [
    "#v4.1\n",
    "#Increasing epochs from 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evx1C9mahdaO",
    "outputId": "3bbba698-de6d-4371-d5b5-3bc6a65818bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GPU status...\n",
      "Wed Mar  5 15:23:14 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P0             28W /   70W |     102MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "GPU is enabled.\n",
      "Found 36028 images belonging to 8 classes.\n",
      "Found 778 images belonging to 8 classes.\n",
      "Classes: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "Loading previous checkpoint...\n",
      "Resuming training from epoch 21 to 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - accuracy: 0.5055 - loss: 1.3164\n",
      "Epoch 21: val_accuracy improved from -inf to 0.52442, saving model to /content/best_augmented_model_4.1.keras\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m886s\u001b[0m 316ms/step - accuracy: 0.5055 - loss: 1.3164 - val_accuracy: 0.5244 - val_loss: 1.2915 - learning_rate: 5.0000e-06\n",
      "Epoch 22/25\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.5091 - loss: 1.3111\n",
      "Epoch 22: val_accuracy improved from 0.52442 to 0.53985, saving model to /content/best_augmented_model_4.1.keras\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 261ms/step - accuracy: 0.5091 - loss: 1.3111 - val_accuracy: 0.5398 - val_loss: 1.2779 - learning_rate: 5.0000e-06\n",
      "Epoch 23/25\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.5099 - loss: 1.3079\n",
      "Epoch 23: val_accuracy did not improve from 0.53985\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 257ms/step - accuracy: 0.5099 - loss: 1.3079 - val_accuracy: 0.5270 - val_loss: 1.3025 - learning_rate: 5.0000e-06\n",
      "Epoch 24/25\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.5162 - loss: 1.2901\n",
      "Epoch 24: val_accuracy did not improve from 0.53985\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 257ms/step - accuracy: 0.5162 - loss: 1.2901 - val_accuracy: 0.5308 - val_loss: 1.2542 - learning_rate: 5.0000e-06\n",
      "Epoch 25/25\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.5167 - loss: 1.2839\n",
      "Epoch 25: val_accuracy did not improve from 0.53985\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 256ms/step - accuracy: 0.5167 - loss: 1.2840 - val_accuracy: 0.5193 - val_loss: 1.2858 - learning_rate: 5.0000e-06\n",
      "Model saved at epoch 25\n",
      "Resuming training from epoch 26 to 30\n",
      "Epoch 26/30\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.5221 - loss: 1.2851\n",
      "Epoch 26: val_accuracy improved from 0.53985 to 0.56427, saving model to /content/best_augmented_model_4.1.keras\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 261ms/step - accuracy: 0.5221 - loss: 1.2852 - val_accuracy: 0.5643 - val_loss: 1.2491 - learning_rate: 5.0000e-06\n",
      "Epoch 27/30\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.5178 - loss: 1.2791\n",
      "Epoch 27: val_accuracy did not improve from 0.56427\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 257ms/step - accuracy: 0.5178 - loss: 1.2791 - val_accuracy: 0.5398 - val_loss: 1.2586 - learning_rate: 5.0000e-06\n",
      "Epoch 28/30\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.5217 - loss: 1.2655\n",
      "Epoch 28: val_accuracy did not improve from 0.56427\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m577s\u001b[0m 256ms/step - accuracy: 0.5217 - loss: 1.2655 - val_accuracy: 0.5373 - val_loss: 1.2516 - learning_rate: 5.0000e-06\n",
      "Epoch 29/30\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.5271 - loss: 1.2723\n",
      "Epoch 29: val_accuracy did not improve from 0.56427\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m582s\u001b[0m 258ms/step - accuracy: 0.5271 - loss: 1.2723 - val_accuracy: 0.5219 - val_loss: 1.2627 - learning_rate: 2.5000e-06\n",
      "Model saved at epoch 30\n",
      "Resuming training from epoch 31 to 35\n",
      "Epoch 31/35\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.5224 - loss: 1.2865\n",
      "Epoch 31: val_accuracy did not improve from 0.56427\n",
      "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 259ms/step - accuracy: 0.5224 - loss: 1.2865 - val_accuracy: 0.5553 - val_loss: 1.2495 - learning_rate: 2.5000e-06\n",
      "Epoch 32/35\n",
      "\u001b[1m2055/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m50s\u001b[0m 256ms/step - accuracy: 0.5203 - loss: 1.2748"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# **Check GPU Status**\n",
    "print(\"Checking GPU status...\")\n",
    "!nvidia-smi\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"GPU is enabled.\")\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# **Paths**\n",
    "PROCESSED_PATH = \"/content/AffectNet_Preprocessed/\"\n",
    "checkpoint_path = \"/content/best_augmented_model_4.1.keras\"  # **Path to uploaded model**\n",
    "\n",
    "# **Data Augmentation**\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "# **Load Dataset**\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PROCESSED_PATH + \"train\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    PROCESSED_PATH + \"val\",  # **Fixed: Load validation set properly**\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# **Get Class Labels**\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "print(\"Classes:\", class_labels)\n",
    "\n",
    "# **Load Saved Model to Resume Training**\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading previous checkpoint...\")\n",
    "    model = load_model(checkpoint_path)\n",
    "    epochs_completed = 20  # Resume from epoch 21\n",
    "else:\n",
    "    print(\"No checkpoint found. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# **Callbacks**\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor=\"val_accuracy\", verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# **Resume Training from Epoch 21 to 40**\n",
    "max_epochs = 40\n",
    "\n",
    "for epoch_range in range(epochs_completed + 1, max_epochs + 1, 5):\n",
    "    print(f\"Resuming training from epoch {epoch_range} to {min(epoch_range + 4, max_epochs)}\")\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=min(epoch_range + 4, max_epochs),\n",
    "        initial_epoch=epoch_range - 1,\n",
    "        callbacks=[early_stop, checkpoint, reduce_lr]\n",
    "    )\n",
    "\n",
    "    # **Save Model Progress After Each Loop**\n",
    "    model.save(checkpoint_path)\n",
    "    print(f\"Model saved at epoch {epoch_range + 4}\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
